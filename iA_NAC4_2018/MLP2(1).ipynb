{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLP\n",
    "\n",
    "Uma das formas mais tradicionais de redes neurais é a MLP, ou Multilayer Perceptron. \n",
    "A MLP é um algoritmo de aprendizado supervisionado que aprende uma função que relaciona entradas e saídas através do seu treinamento em um conjunto de dados.\n",
    "\n",
    "Tipicamente, uma MLP é formada por:\n",
    "- uma camada de entrada, que corresponde aos M atributos a serem usados para a classificação\n",
    "- uma camada de saída, que possui um neurônio para cada uma das N classes em que a amostra de atributos será classificada\n",
    "- uma ou mais camadas ocultas de neurônios, responsáveis por tornar a MLP um classificador não linear\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A instanciação de uma MLP\n",
    "\n",
    "Para criarmos uma MLP no `scikit-learn` usamos a classe `MLPClassifier` do pacote `sklearn.neural_network` através do código:\n",
    "```python\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "```\n",
    "A instanciação de um objeto representando a MLP depende da definição de **hiperparâmetros**, ou seja parâmetros da rede que não serão aprendidos durante a fase de treinamento. Até agora, os hiperparâmetros que aprendemos são:\n",
    "\n",
    "### 1. Número de camadas ocultas\n",
    "Indica quandas camadas ocultas serão usadas na rede neural. Há provas matemáticas de que uma rede do tipo MLP com função de ativação sigmóide pode aproximar qualquer função contínua com apenas uma única camada oculta de neurônios, porém para funções descontínuas, duas são necessárias (ver livro do Russel, pág. 720). Na prática, para dados simples, não precisamos de mais do que uma camada, e partimos para a decisão do número de neurônios nessa camada. O inconveniente dessa abordagem é que o número de neurônios necessários para realizar a classificação corretamente pode ter que crescer muito dependendo da complexidade dos dados. \n",
    "\n",
    "### 2. Número de neurônios por camada oculta\n",
    "No caso de escolhermos usar apenas uma camada oculta, é preciso escolher o número de neurônios a ser empregado.\n",
    "Há algumas \"receitas de bolo\" para essa decisão, porém a melhor forma de determinar esse parâmetro é realizar diversos testes com os dados de treinamento, através da *validação cruzada*. Uma dessas receitas de bolo indica que o número de neurônios da camada oculta seja o dobro do número de elementos de entrada mais um, que nós podemos usar como ponto de partida para a realização de testes.\n",
    "\n",
    "### 3. Função de ativação $f(a)$\n",
    "A função de ativação processa a entrada líquida do neurônio, resultante do somatório das entradas multiplicadas pelos pesos.\n",
    "Para a tarefa de classificação, as funções de ativação mais comumente empregadas são:\n",
    "- Logística ou sigmóide: $f(a)=\\frac{1}{1+e^{-a}}$\n",
    "<img width=\"400px\" src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/8/88/Logistic-curve.svg/1920px-Logistic-curve.svg.png\"/>\n",
    "- Tangente hiperbólica: $f(a) = tanh(a) = \\frac{e^a - e^{-a}}{e^a + e^{-a}}$\n",
    "<img width=\"400px\" src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/8/87/Hyperbolic_Tangent.svg/250px-Hyperbolic_Tangent.svg.png\">\n",
    "\n",
    "- Linear retificada (ReLU): $f(a) = \\begin{cases} a, & \\mbox{se } a>=0 \\\\ 0, & \\mbox{caso contrário } \\end{cases}$\n",
    "<img width=\"400px\" src=\"http://ml4a.github.io/images/figures/relu.png\">\n",
    "\n",
    "\n",
    "Há também parâmetros relacionados ao processo de treinamento:\n",
    "- Taxa de aprendizado $\\eta$: valor maior do que zero que representa a velocidade em que ocorre o aprendizado\n",
    "- *Batch size*: número de amostras a serem usadas a cada atualização de pesos\n",
    "- Algoritmo de otimização: embora o algoritmo Backpropagation e suas variações seja o mais empregado, há outros algoritmos disponíveis para o trienamento. Aqui vamos trabalhar com o Backpropagation tradicional.\n",
    "- Número de épocas: determina quantas épocas, ou seja, quantas vezes cada amostra de entrada será utilizada, mesmo que o algoritmo de otimização não tenha encontrado um resultado confiável\n",
    "\n",
    "Dessa forma, a instanciação da nossa rede MLP fica:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "# Definição dos hiperparâmetros\n",
    "# Função de ativação: pode ser uma dentre: {‘identity’, ‘logistic’, ‘tanh’, ‘relu’}, default ‘relu’\n",
    "activation = 'logistic' \n",
    "# Tamanhos das camadas ocultas\n",
    "hidden_layer_sizes = [4] # Apenas uma camada com quatro neurônios\n",
    "# Algoritmo de otimização: pode ser um dentre {‘lbfgs’, ‘sgd’, ‘adam’}, default ‘adam’\n",
    "solver = 'sgd' #Stochastic Gradient Descent\n",
    "# Valor da taxa de aprendizado, default 0.001\n",
    "learning_rate =  0.03\n",
    "# Batch size, default é 'auto', ou seja, min(200, n_samples)\n",
    "batch_size=50\n",
    "# Número máximo de épocas, default 200\n",
    "max_iter = 1000\n",
    "mlp = MLPClassifier(hidden_layer_sizes=hidden_layer_sizes,\n",
    "                    activation=activation, solver=solver,\n",
    "                    learning_rate_init=learning_rate,\n",
    "                    batch_size=batch_size,\n",
    "                    max_iter=max_iter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Treinamento da MLP\n",
    "\n",
    "Executar a classificação com MLP através do pacote de Machine Learning mais conhecido do Python, o `scikit-learn'.\n",
    "\n",
    "Passos que seguiremos:\n",
    "1. Carregar o arquivo de dados do iris dataset em um DataFrame do `pandas`.\n",
    "2. Normalizar os dados de entrada\n",
    "3. Separar aleatoriamente o dataframe em amostras de treinamento e de validação\n",
    "4. Treinar o classificador MLP instanciado \n",
    "5. Exeutar o classificador e visualizar os resultados obtidos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importando o módulo pandas, e invocando como pd\n",
    "import pandas as pd\n",
    "\n",
    "# 1. Carregar o arquivo na variável df\n",
    "url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data\"\n",
    "header = ['sepal_length', 'sepal_width', 'petal_length', 'petal_width', 'species']\n",
    "df = pd.read_csv(url, header=None, names=header)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.Encontrar um dataframe numérico normalizado\n",
    "df_norm = df[['sepal_length', 'sepal_width', 'petal_length', 'petal_width']]\n",
    "# Execução da normalização, onde subtraímos a média (mean) e dividimos\n",
    "# pelo desvio padrão (standard deviation - std) coluna a coluna\n",
    "df_norm = (df_norm - df_norm.mean())/df_norm.std()\n",
    "# Aqui acrescentamos a coluna de espécies\n",
    "df_norm['species'] = df['species']\n",
    "\n",
    "# 3. Separando as amostras de treinamento e validação\n",
    "frac_validacao = 0.2 # Separamos 20% para a validação\n",
    "# Primeiro embaralhamos o dataframe...\n",
    "from sklearn.utils import shuffle\n",
    "df_shuffle = shuffle(df_norm)\n",
    "# ... depois separamos em porção de treinamento e validação\n",
    "import math\n",
    "# Ponto de corte\n",
    "icut = math.floor(frac_validacao * df_shuffle.shape[0])\n",
    "#Divisão do data frame\n",
    "df_valid = df_shuffle.iloc[:icut, :]\n",
    "df_treino  = df_shuffle.iloc[icut:, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Treinar o classificador MLP instanciado anteriormente, seprando as entradas e saídas \n",
    "entradas = df_treino[['sepal_length', 'sepal_width', 'petal_length', 'petal_width']]\n",
    "saidas = df_treino['species']\n",
    "mlp.fit(entradas, saidas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Uma observação sobre a sintaxe dos classificadores do `scikit-learn`\n",
    "- O método fit(X,Y) recebe uma matriz ou dataframe X onde cada linha é uma amostra de aprendizado, e um array Y contendo as saídas esperadas do classificador, seja na forma de texto ou de inteiros\n",
    "- O método predict(X) recebe uma matriz ou dataframe X onde cada linha é uma amostra de teste, retornando um array de classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Executar o classificador nas amostras de validação\n",
    "entradas_valid = df_valid[['sepal_length', 'sepal_width', 'petal_length', 'petal_width']]\n",
    "classes = mlp.predict(entradas_valid)\n",
    "\n",
    "#Cálculo do erro\n",
    "import numpy as np\n",
    "# Conta quantas amostras foram classificadas de modo errado\n",
    "erro_total = np.sum(classes != df_valid['species'])\n",
    "# Encontra o ero médio por amostra\n",
    "erro_medio = erro_total/df_valid.shape[0]\n",
    "print(\"Erro médio de classificação: \", erro_medio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8. Adendo: visualizando o resultado usando o seaborn e matplotlib\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "\n",
    "def visualizar_resultado(data_learn, data_valid, colunas_dados, coluna_classes, classifier):\n",
    "    h = 0.2 # Step size in the mesh\n",
    "    #Definimos a faixa de valores de X e Y \n",
    "    X = data_learn[colunas_dados[0]]\n",
    "    Y = data_learn[colunas_dados[1]]\n",
    "    x_min = min(X.min(), data_valid[colunas_dados[0]].min()) - .5\n",
    "    x_max = max(X.max(), data_valid[colunas_dados[0]].max()) + .5\n",
    "    y_min = min(Y.min(), data_valid[colunas_dados[0]].min()) - .5\n",
    "    y_max = max(Y.max(), data_valid[colunas_dados[0]].max()) + .5\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n",
    "  \n",
    "    # Treina o classificador para as features de visualização\n",
    "    classifier.fit(data_learn[colunas_dados], data_learn[coluna_classes])\n",
    "    # Resultado da classificação\n",
    "    classes = classifier.predict(data_valid[colunas_dados])\n",
    "    \n",
    "    # Zonas de classificação\n",
    "    i = 0\n",
    "    cores = ['blue', 'orange', 'green']\n",
    "    for cls in np.unique(data_learn[coluna_classes]):\n",
    "        Z = classifier.predict_proba(np.c_[xx.ravel(), yy.ravel()])[:, i]\n",
    "        # Put the result into a contour plot\n",
    "        Z = Z.reshape(xx.shape)\n",
    "        plt.contour(xx, yy, Z, 1, colors=cores[i%len(cores)])\n",
    "        i += 1\n",
    "\n",
    "    # Plot also the training points\n",
    "    unique_classes = np.unique(data_learn[coluna_classes])\n",
    "    sns.scatterplot(*colunas_dados, hue=coluna_classes, hue_order=unique_classes, data=data_learn)\n",
    "    # and testing points\n",
    "    sns.scatterplot(*colunas_dados, hue=classes, hue_order=unique_classes, data=data_valid, marker='+', legend=False)\n",
    "    \n",
    "    plt.xlim(x_min, x_max)\n",
    "    plt.ylim(y_min, y_max)\n",
    "    \n",
    "\n",
    "visualizar_resultado(df_treino, entradas_valid, ['petal_length', 'petal_width'], 'species', mlp)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
